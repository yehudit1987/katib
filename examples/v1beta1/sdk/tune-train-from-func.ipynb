{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tune and Train with Kubeflow Katib and Training Operator\n",
    " \n",
    "In this Notebook we are going to do the following:\n",
    "\n",
    "- Train Tensorflow model using Kubeflow Notebook.\n",
    "- Improve the model HyperParameters with [Kubeflow Katib](https://www.kubeflow.org/docs/components/katib/overview/).\n",
    "- Use [Multi Worker Mirrored Strategy](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) to distributively train the model with [Kubeflow TFJob](https://www.kubeflow.org/docs/components/training/tftraining/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Install Kubeflow Python SDKs\n",
    "\n",
    "You need to install Tensorflow package and Kubeflow SDKs to run this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.16.1\n",
    "# TODO (andreyvelich): Change to release version when SDK with the new APIs is published.\n",
    "!pip install git+https://github.com/kubeflow/katib.git#subdirectory=sdk/python/v1beta1\n",
    "!pip install git+https://github.com/kubeflow/training-operator.git#subdirectory=sdk/python"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Train Script for CNN Model\n",
    "\n",
    "This is simple **Convolutional Neural Network (CNN)** model for recognizing hand-written digits using [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_mnist_model(parameters):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import logging\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "        datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logging.info(\"--------------------------------------------------------------------------------------\")\n",
    "    logging.info(f\"Input Parameters: {parameters}\")\n",
    "    logging.info(\"--------------------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "\n",
    "    # Get HyperParameters from the input params dict.\n",
    "    lr = float(parameters[\"lr\"])\n",
    "    num_epoch = int(parameters[\"num_epoch\"])\n",
    "\n",
    "    # Set dist parameters and strategy.\n",
    "    is_dist = parameters[\"is_dist\"]\n",
    "    num_workers = parameters[\"num_workers\"]\n",
    "    batch_size_per_worker = 64\n",
    "    batch_size_global = batch_size_per_worker * num_workers\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
    "        communication_options=tf.distribute.experimental.CommunicationOptions(\n",
    "            implementation=tf.distribute.experimental.CollectiveCommunication.RING\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Callback class for logging training.\n",
    "    # Katib parses metrics in this format: <metric-name>=<metric-value>.\n",
    "    class CustomCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            logging.info(\n",
    "                \"Epoch {}/{}. accuracy={:.4f} - loss={:.4f}\".format(\n",
    "                    epoch+1, num_epoch, logs[\"accuracy\"], logs[\"loss\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Prepare MNIST Dataset.\n",
    "    def mnist_dataset(batch_size):\n",
    "        (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "        x_train = x_train / np.float32(255)\n",
    "        y_train = y_train.astype(np.int64)\n",
    "        train_dataset = (\n",
    "            tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            .shuffle(60000)\n",
    "            .repeat()\n",
    "            .batch(batch_size)\n",
    "        )\n",
    "        return train_dataset\n",
    "\n",
    "    # Build and compile CNN Model.\n",
    "    def build_and_compile_cnn_model():\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "                tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "                tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(10),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    # Download Dataset.\n",
    "    dataset = mnist_dataset(batch_size_global)\n",
    "\n",
    "    # For dist strategy we should build model under scope().\n",
    "    if is_dist:\n",
    "        logging.info(\"Running Distributed Training\")\n",
    "        logging.info(\"--------------------------------------------------------------------------------------\\n\\n\")\n",
    "        with strategy.scope():\n",
    "            model = build_and_compile_cnn_model()\n",
    "    else:\n",
    "        logging.info(\"Running Single Worker Training\")\n",
    "        logging.info(\"--------------------------------------------------------------------------------------\\n\\n\")\n",
    "        model = build_and_compile_cnn_model()\n",
    "    \n",
    "    # Start Training.\n",
    "    model.fit(\n",
    "        dataset,\n",
    "        epochs=num_epoch,\n",
    "        steps_per_epoch=70,\n",
    "        callbacks=[CustomCallback()],\n",
    "        verbose=0,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Training Locally in the Notebook\n",
    "\n",
    "We are going to download MNIST Dataset and start local training.\n",
    "\n",
    "Also, set `Epochs = 2` to reduce training time and avoid CPU overload. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set Parameters for Local Training.\n",
    "parameters = {\n",
    "    \"lr\": \"0.1\",\n",
    "    \"num_epoch\": \"2\",\n",
    "    \"is_dist\": False,\n",
    "    \"num_workers\": 1\n",
    "}\n",
    "\n",
    "# Train Model locally in the Notebook.\n",
    "train_mnist_model(parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start Model Tuning with Katib\n",
    "\n",
    "If you want to improve your model, you can run HyperParameter tuning with Katib.\n",
    "\n",
    "The following example uses **Covariance Matrix Adaptation Evolution Strategy (CMA-ES)** algorithm to tune HyperParameters.\n",
    "\n",
    "We are going to tune `learning rate` and `number of epochs`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import kubeflow.katib as katib\n",
    "\n",
    "# Set parameters with their distribution for HyperParameter Tuning with Katib.\n",
    "parameters = {\n",
    "    \"lr\": katib.search.double(min=0.1, max=0.2),\n",
    "    \"num_epoch\": katib.search.int(min=10, max=15),\n",
    "    \"is_dist\": False,\n",
    "    \"num_workers\": 1\n",
    "}\n",
    "# Start the Katib Experiment.\n",
    "exp_name = \"tune-mnist\"\n",
    "namespace=\"kubeflow\"\n",
    "katib_client = katib.KatibClient(namespace=namespace)\n",
    "\n",
    "katib_client.tune(\n",
    "    name=exp_name,\n",
    "    namespace=namespace,\n",
    "    objective=train_mnist_model, # Objective function.\n",
    "    parameters=parameters, # HyperParameters to tune.\n",
    "    algorithm_name=\"cmaes\", # Alorithm to use.\n",
    "    objective_metric_name=\"accuracy\", # Katib is going to optimize \"accuracy\".\n",
    "    additional_metric_names=[\"loss\"], # Katib is going to collect these metrics in addition to the objective metric.\n",
    "    max_trial_count=12, # Trial Threshold.\n",
    "    parallel_trial_count=2,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Access to Katib UI\n",
    "\n",
    "You can check created Experiment in the Katib UI.\n",
    "\n",
    "![Screenshot 2022-09-12 at 20.06.23.png](attachment:cdaf463d-28b3-4a98-bb4c-9613ca1bfa50.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get the Best HyperParameters from the Katib Experiment\n",
    "\n",
    "You can get the best HyperParameters from the most optimal Katib Trial."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "status = katib_client.is_experiment_succeeded(exp_name, namespace=namespace)\n",
    "print(f\"Katib Experiment is Succeeded: {status}\\n\")\n",
    "time.sleep(180)\n",
    "\n",
    "best_hps = katib_client.get_optimal_hyperparameters(exp_name, namespace=namespace)\n",
    "\n",
    "if best_hps != None:\n",
    "    print(\"Current Optimal Trial\\n\")\n",
    "    print(best_hps)\n",
    "    \n",
    "    for hp in best_hps.parameter_assignments:\n",
    "        if hp.name == \"lr\":\n",
    "            best_lr = hp.value\n",
    "        else:\n",
    "            best_num_epoch = hp.value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start Distributive Training with TFJob\n",
    "\n",
    "Use the best HyperParameters (`learning rate` and `number of epochs`) from the Katib Experiment and run the TFJob."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from kubeflow.training import TrainingClient\n",
    "\n",
    "# Set Parameters for Distributed Training with TFJob.\n",
    "parameters = {\n",
    "    \"lr\": best_lr,\n",
    "    \"num_epoch\": best_num_epoch,\n",
    "    \"is_dist\": True,\n",
    "    \"num_workers\": 5\n",
    "}\n",
    "\n",
    "# Start TFJob Training.\n",
    "tfjob_name = \"train-mnist\"\n",
    "tfjob_client = TrainingClient()\n",
    "\n",
    "#create_tfjob_from_func\n",
    "tfjob_client.create_job(\n",
    "    name=tfjob_name,\n",
    "    namespace=namespace,\n",
    "    job_kind=\"TFJob\",\n",
    "    train_func=train_mnist_model,\n",
    "    parameters=parameters, # Input parameters for the train function.\n",
    "    num_workers=5, # How many TFJob Workers will be run.\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get TFJob Status and Training Logs\n",
    "\n",
    "You can check the TFJob status and logs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"TFJob status: {tfjob_client.get_job_conditions(tfjob_name, namespace=namespace, job_kind='TFJob')}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfjob_client.get_job_logs(name=tfjob_name, namespace=namespace, is_master=True, follow=True, job_kind=\"TFJob\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Delete Katib Experiment and TFJob\n",
    "\n",
    "When jobs are finished, you can delete the resources."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "katib_client.delete_experiment(exp_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfjob_client.delete_job(tfjob_name, namespace=namespace, job_kind=\"TFJob\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kubeflow Tensorflow",
   "language": "python",
   "name": "kurek2tw49o8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}